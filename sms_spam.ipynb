{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d770f614",
   "metadata": {},
   "source": [
    "### Email Spammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1c7c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "print(df.shape);\n",
    "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)\n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c9c8a",
   "metadata": {},
   "source": [
    "#### 1.Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3639131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5169, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "df['target'] = encoder.fit_transform(df['target'])\n",
    "df.isnull().sum()\n",
    "df.duplicated().sum()\n",
    "df = df.drop_duplicates(keep=\"first\")\n",
    "df.duplicated().sum()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd25fb",
   "metadata": {},
   "source": [
    "#### 2.EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df['text'].apply(len)\n",
    "df['num_char'] = df['text'].apply(len) \n",
    "\n",
    "df['num_words'] = df['text'].apply(lambda x :len(nltk.word_tokenize(x)))\n",
    "df['num_sent'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "df[['num_char','num_words','num_sent']].describe()\n",
    "\n",
    "# for ham\n",
    "df[df['target'] == 0][['num_char','num_words','num_sent']].describe()\n",
    "# for spam\n",
    "df[df['target'] == 1][['num_char','num_words','num_sent']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6b881",
   "metadata": {},
   "source": [
    "#### 3.Data preprocessing  or text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem('dancing')\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()        \n",
    "\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]    \n",
    "    y.clear()\n",
    "\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "\n",
    "    return \" \".join(y)\n",
    "    \n",
    "\n",
    "transform_text(\"heelo my name is priyank\")\n",
    "df['transformed_text'] = df['text'].apply(transform_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eab51",
   "metadata": {},
   "source": [
    "#### 4.Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width = 500,height=500,min_font_size=10,background_color=\"white\")\n",
    "\n",
    "spam_wc = wc.generate(df[df['target'] == 1][\"transformed_text\"].str.cat(sep = \" \"))\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus = []\n",
    "for msg in df[df['target'] == 1]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)\n",
    "\n",
    "len(spam_corpus)        \n",
    "\n",
    "from collections import Counter\n",
    "spam = pd.DataFrame(Counter(spam_corpus).most_common(30))\n",
    "\n",
    "\n",
    "Ham_corpus = []\n",
    "for msg in df[df['target'] == 0]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        Ham_corpus.append(word)    \n",
    "\n",
    "len(Ham_corpus)\n",
    "        \n",
    "from collections import Counter\n",
    "ham = pd.DataFrame(Counter(Ham_corpus).most_common(30))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f6e07",
   "metadata": {},
   "source": [
    "#### 5.Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c10e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer \n",
    "\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "\n",
    "X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "\n",
    "\n",
    "X.shape\n",
    "y = df['target'].values\n",
    "y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(precision_score(y_test,y_pred1))\n",
    "\n",
    "mnb.fit(X_train,y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))\n",
    "\n",
    "bnb.fit(X_train,y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
